{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Phi-3.5-mini-instruct](https://huggingface.co/microsoft/Phi-3.5-mini-instruct)"
      ],
      "metadata": {
        "id": "awMieknXJ11O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install required packages and Auto - Restart Session\n",
        "!pip install flash_attn==2.5.8\n",
        "# torch==2.3.1\n",
        "!pip install accelerate==0.31.0\n",
        "!pip install transformers==4.43.0\n",
        "!pip install gradio==4.36.1\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "import time\n",
        "time.sleep(5)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mc7gLJLEEZxu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <-- Play Audio to active the colab Notebook { display-mode: \"form\" }\n",
        "\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive, then run the cell below</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "wrkIqzSnFlhw",
        "outputId": "cd351934-f8d7-4fb3-b29e-2dc55bd0de22"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<b>Press play on the music player to keep the tab alive, then run the cell below</b><br/>\n",
              "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Phi-3.5-mini-instruct Model\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import gc\n",
        "torch.random.manual_seed(0)\n",
        "import subprocess\n",
        "import time\n",
        "def is_gpu_memory_over_limit(limit_gb=14.5):\n",
        "    # Run nvidia-smi and capture the output\n",
        "    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'],\n",
        "                            stdout=subprocess.PIPE, text=True)\n",
        "\n",
        "    # Split the result into lines (for each GPU if there are multiple)\n",
        "    memory_used_mb_list = result.stdout.strip().splitlines()\n",
        "\n",
        "    # Convert memory used from MB to GB and check each GPU's memory usage\n",
        "    for i, memory_used_mb in enumerate(memory_used_mb_list):\n",
        "        memory_used_gb = int(memory_used_mb) / 1024.0\n",
        "        print(f\"GPU {i}: Current memory allocated: {memory_used_gb:.2f} GB\")\n",
        "        if memory_used_gb > limit_gb:\n",
        "            print(f\"GPU {i} memory usage exceeds {limit_gb} GB.\")\n",
        "            return True\n",
        "\n",
        "    print(\"GPU memory usage is within safe limits.\")\n",
        "    return False\n",
        "\n",
        "model=None\n",
        "tokenizer=None\n",
        "def load_model():\n",
        "  global model,tokenizer\n",
        "  try:\n",
        "    if tokenizer is not None:\n",
        "      del tokenizer\n",
        "      tokenizer=None\n",
        "    if model is not None:\n",
        "      del model\n",
        "      model=None\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Free GPU memeory\")\n",
        "  except:\n",
        "    pass\n",
        "  time.sleep(2)\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3.5-mini-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "  )\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
        "  return model,tokenizer\n",
        "\n",
        "def phi_Chat(system_role,user_msg):\n",
        "  global model,tokenizer\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": system_role},\n",
        "    {\"role\": \"user\", \"content\": user_msg}\n",
        "   ]\n",
        "\n",
        "  pipe = pipeline(\n",
        "      \"text-generation\",\n",
        "      model=model,\n",
        "      tokenizer=tokenizer,\n",
        "  )\n",
        "\n",
        "  generation_args = {\n",
        "      \"max_new_tokens\": 500,\n",
        "      \"return_full_text\": False,\n",
        "      \"temperature\": 0.0,\n",
        "      \"do_sample\": False,\n",
        "  }\n",
        "\n",
        "  output = pipe(messages, **generation_args)\n",
        "  response=output[0]['generated_text']\n",
        "  return response.strip()\n",
        "\n",
        "model,tokenizer=load_model()\n",
        "\n",
        "def run_phi(system_role,user_msg):\n",
        "  global model,tokenizer\n",
        "  print(f\"System Role:{system_role}\\nuser Message:{user_msg}\")\n",
        "  if is_gpu_memory_over_limit(14):\n",
        "    tokenizer,model=load_model()\n",
        "  phi_reply=phi_Chat(system_role,user_msg)\n",
        "  print(f\"Response:{phi_reply}\")\n",
        "  return phi_reply\n",
        "\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BjISfWyRGXxe"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test Phi-3.5-mini-instruct\n",
        "system_role= \"You are a helpful AI assistant.\"  # @param {type: \"string\"}\n",
        "user_msg = \"Who are you?\"  # @param {type: \"string\"}\n",
        "llama_reply=run_phi(system_role,user_msg)\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(llama_reply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "VJSlxiW-JAcg",
        "outputId": "b24e4d7c-985e-4b02-da53-9b51da4ec72b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am Phi, created by Microsoft. I'm an AI digital assistant designed to help answer questions and provide information to the best of my ability.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Gradio API Interface\n",
        "username = 'admin'  # @param {type: \"string\"}\n",
        "password = 'admin'  # @param {type: \"string\"}\n",
        "Debug = True  # @param {type: \"boolean\"}\n",
        "\n",
        "import gradio as gr\n",
        "gradio_examples = [[\"You are a helpful AI assistant.\",\"Who are you?\"]]\n",
        "gradio_input=[gr.Textbox(label=\"System Role\"),gr.Textbox(label=\"User Message\")]\n",
        "gradio_output=[gr.Textbox(label=\"Hermes-3-Llama-3.1-8B Response\")]\n",
        "gradio_interface = gr.Interface(fn=run_phi, inputs=gradio_input,outputs=gradio_output , title=\"Hermes-3-Llama-3.1-8B\",examples=gradio_examples)\n",
        "gradio_interface.queue().launch(share=True,debug=Debug,auth=(username, password))\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EV8QTkt-MdML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n68MJ_5UM_Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b-_3M8pUgxft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Gradio Chat Interface\n",
        "import time\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "def slow_echo(user_msg, history):\n",
        "    system_role= \"You are a helpful Assistant.\"  # @param {type: \"string\"}\n",
        "    message=run_phi(system_role,user_msg)\n",
        "    for i in range(len(message)):\n",
        "        time.sleep(0.05)\n",
        "        yield \"\" + message[: i + 1]\n",
        "\n",
        "\n",
        "demo = gr.ChatInterface(slow_echo)\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "cellView": "form",
        "id": "dbVrezeyMmyl",
        "outputId": "e277c0e1-a94a-4db0-c793-06648e1aee7f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://f02a99b1f3ea872241.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f02a99b1f3ea872241.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}